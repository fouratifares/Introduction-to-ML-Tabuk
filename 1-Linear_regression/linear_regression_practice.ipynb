{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp0xEaxQ9IdQ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the required libraries (needed when running outside colab where the environment doesn't come pre-loaded with libraries)\n",
        "\n",
        "%pip install numpy\n",
        "%pip install scikit-learn\n",
        "%pip install matplotlib\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "GMpV5X0P9RJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sklearn has implementations of multiple types of models. We'll be using LinearRegression API in it\n",
        "# For documentation on LinearRegression, visit here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "-h6xg45AYUmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contents:\n",
        "\n",
        "1. Implement 1 polynomial degree Linear Regression model from scratch (using numpy)\n",
        "2. Implement the same model using sklearn\n",
        "3. Take a complex function and try fitting a multi-polynomial degree Linear regression model on it.\n",
        "\n",
        "\n",
        "You need to know:\n",
        "\n",
        "1. **numpy** (for impelementation)\n",
        "2. a little bit of **matplotlib** (for visualization)\n",
        "\n",
        "\n",
        "Good to have knowledge of:\n",
        "\n",
        "1. Sklearn (details of the functions is given anyways)"
      ],
      "metadata": {
        "id": "37bi0tehXm-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Implementing Linear Regression from Scratch (Using numpy)"
      ],
      "metadata": {
        "id": "Shzta92y9oZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate a Simple Problem"
      ],
      "metadata": {
        "id": "XhO4sK1Yy3-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make some custom points (which would act as our dataset)\n",
        "# starting with a function with highest polynomial degree of 1\n",
        "\n",
        "# y = w0 + w1*x1\n",
        "\n",
        "w0 = 12  # bias\n",
        "w1 = 15  # first degree co-efficient\n",
        "noise_scale_factor = 5  # the higher this is, the rougher and farther the noisy output is from the best fit\n",
        "num_points = 50  # number of samples in data\n",
        "\n",
        "x_points = np.linspace(0, 1, num_points)  # num_points points from 0 to 1\n",
        "y_points_no_noise = w0 + w1*x_points  # The actual features which we'd feed to model will have slight noise\n",
        "noise = noise_scale_factor * (np.random.rand(*y_points_no_noise.shape)-0.5)  # Look at the explanation below to see how this is calculated\n",
        "\n",
        "#Question: what happens if we comment the following line? and replace it with this y_points =  y_points_no_noise\n",
        "y_points = y_points_no_noise + noise\n",
        "\n",
        "\n",
        "plt.plot(x_points, y_points_no_noise, label='No noise output (what we want as best fit)')\n",
        "plt.plot(x_points, y_points,'*', label='Noisy output (labels fed to model)')\n",
        "_, ylim_top = plt.ylim()\n",
        "plt.ylim(0, ylim_top)  # 0 bottom makes it easy to visualize bias term\n",
        "\n",
        "plt.xlabel('X-points (features)')\n",
        "plt.ylabel('Y-points (output / labels)')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PwIENVuG99b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Explanation**:\n",
        "\n",
        "```python\n",
        "noise_scale_factor = 5\n",
        "noise = noise_scale_factor * (np.random.rand(*y_points_no_noise.shape)-0.5)\n",
        "y_points = y_points_no_noise + noise\n",
        "```\n",
        "\n",
        "- np.random.rand(*y_points_no_noise) generates an array of random values in range [0, 1) of the same shape as y_points_no_noise\n",
        "- we subtract 0.5 from it to bring it to range [-0.5, 0.5) (so there are also some values below the line after noise is added)\n",
        "- it might be possible that the values of w0, w1 or y_points is big enough for a noise point of [-0.5, 0.5) to not make much dent (for example, if a label is 1000, changing it to 1000.5 or 999.5 doesn't make much different) so we scale it with a scaling factor\n",
        "- The calculated noise is added to noise-less y_points"
      ],
      "metadata": {
        "id": "_zaagbKQq3e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit the Data with a Linear Regression Model"
      ],
      "metadata": {
        "id": "PFVfsqmL0EvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solve for Θ to find our params**\n",
        "\n",
        "We recommend the use of upper case for matrices and lower case for vectors.\n",
        "\n",
        "  X θ = y\n",
        "\n",
        "  Xᵀ(X θ) = Xᵀy  \n",
        "\n",
        "  =>  XᵀX θ = Xᵀy (***`normal equation`***)\n",
        "\n",
        "  => θ = (XᵀX)⁻¹Xᵀy (***model***)\n",
        "\n",
        "Hint: you can use np.linalg.inv, please open documentation if you are not sure"
      ],
      "metadata": {
        "id": "6Fyry0qnoljC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**prepare the features**"
      ],
      "metadata": {
        "id": "jDlvYJJ60R2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "X = [???]\n",
        "X = np.array(X).T  # (m, n) matrix where m is number of samples and n is number of params (including bias). In our case, n is 2\n",
        "theta = ???\n",
        "y_pred = ???\n",
        "\n",
        "\n",
        "\n",
        "#plot\n",
        "plt.plot(x_points, y_points_no_noise, label='data points without the noise (our goal)')\n",
        "plt.plot(x_points, y_points, 'x', label='points in data (with noise)')\n",
        "plt.plot(x_points, y_pred, label='prediction')\n",
        "\n",
        "_, ylim_top = plt.ylim()\n",
        "plt.ylim(0, ylim_top+10)  # 0 bottom makes it easy to visualize bias term (intercept)\n",
        "\n",
        "plt.xlabel('X-points (features)')\n",
        "plt.ylabel('Y-points (output / labels)')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ONfu5C4kpl0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Actual weight values we used: {(w0, w1)}')\n",
        "print(f'Values calculated by Model: {tuple(theta)}')"
      ],
      "metadata": {
        "id": "RsSuO-qfx5gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's now use sklearn to do this"
      ],
      "metadata": {
        "id": "mDMgBfr0XmaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression(fit_intercept=True)\n",
        "model = model.fit(x_points.reshape(-1, 1), y_points)  # fit is used to train the model on the data."
      ],
      "metadata": {
        "id": "U6yN5mOahfDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_points.reshape(-1, 1))  # Predict is used to prediction labels/outputs from feature inputs."
      ],
      "metadata": {
        "id": "9fU8fYejyYk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Normally it's a good idea to predict a separate set of data not seen during training (validation data), but here we're just looking at the basic implementation using sklearn so training data would also do"
      ],
      "metadata": {
        "id": "gNu0XUo04t-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_points, y_points_no_noise, label='data points without the noise (our goal)')\n",
        "plt.plot(x_points, y_points, 'x', label='points in data (with noise)')\n",
        "plt.plot(x_points, y_pred, label='prediction')\n",
        "\n",
        "_, ylim_top = plt.ylim()\n",
        "plt.ylim(0, ylim_top+10)\n",
        "plt.xlabel('X-points (features)')\n",
        "plt.ylabel('Y-points (output / labels)')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7p-tfpPhznBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Expected coefficients/matrices:           {(w0, w1)}')\n",
        "\n",
        "# we have to do [0] on coef_ because coef_ itself is a matrix of all the coeffifients. In our case, coef_ just has one length but it's still an array.\n",
        "# [0] brings out the value from array which means better printing\n",
        "print(f'Sklearn calculated coeffidients/matrices: {(model.intercept_, model.coef_[0])}')"
      ],
      "metadata": {
        "id": "Bn03H6mTzqG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's step up the game with more complex functions"
      ],
      "metadata": {
        "id": "ba-4Ox1r7bN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_scale_factor = 1.5\n",
        "num_points = 100\n",
        "sin_wave_ub = 3  # upper bound of sin wave y output. negative of this value would also be lower bound. (negative value here would make this a cosine wave)\n",
        "\n",
        "x_points = np.linspace(-1, 1, num_points)\n",
        "y_points_no_noise = sin_wave_ub * np.sin(x_points*math.pi)\n",
        "\n",
        "noise = noise_scale_factor * (np.random.rand(*y_points_no_noise.shape)-0.5)\n",
        "y_points = y_points_no_noise + noise\n",
        "\n",
        "plt.plot(x_points, y_points_no_noise, label='No noise output')\n",
        "plt.plot(x_points, y_points, label='Noisy output (labels fed to model)')\n",
        "\n",
        "plt.xlabel('X-points (features)')\n",
        "plt.ylabel('Y-points (output / labels)')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jYpUaxrn1VB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Custom fitting with variable number of bases"
      ],
      "metadata": {
        "id": "2Ut2OpDhZTFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom calculating best fit\n",
        "\n",
        "\n",
        "#TODO: try different values for these\n",
        "min_num_bases = 0\n",
        "max_num_bases = 6\n",
        "\n",
        "\n",
        "assert min_num_bases <= max_num_bases, \"min_num_bases must be lesser or equal to max_num_bases. :|\"\n",
        "\n",
        "bases_preds = []\n",
        "\n",
        "for num_bases in range(min_num_bases, max_num_bases+1):\n",
        "\n",
        "  A = [x_points**i for i in range(num_bases+1)].  #TODO: what does this line do\n",
        "  A = np.array(A).T\n",
        "\n",
        "  #TODO\n",
        "  w = ?\n",
        "  y_pred = ?\n",
        "  bases_preds.append(?)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(len(bases_preds), 1, figsize=(20, 10*len(bases_preds)))\n",
        "\n",
        "for i, (num_bases, preds) in enumerate(zip(range(min_num_bases, max_num_bases+1), bases_preds)):\n",
        "\n",
        "    axes[i].set_title(f'Num Bases = {num_bases}')\n",
        "\n",
        "    # Visualize the results\n",
        "    axes[i].plot(x_points, y_points_no_noise, label='data points without the noise (our goal)')\n",
        "    axes[i].plot(x_points, y_points, 'x',     label='points in data (with noise)')\n",
        "    axes[i].plot(x_points, bases_preds[i],    label='prediction')\n",
        "\n",
        "    axes[i].set_xlabel('X-points (features)')\n",
        "    axes[i].set_ylabel('Y-points (output / labels)')\n",
        "\n",
        "    axes[i].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GTAmcROa-zg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DV1p1u_CYPPL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}